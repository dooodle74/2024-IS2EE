{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import init\n",
    "\n",
    "from torchvision.models.resnet import BasicBlock, ResNet\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import torch.utils.data as data_utils\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "def default_loader(path):\n",
    "    return Image.open(path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models.resnet import BasicBlock, ResNet\n",
    "from torch.nn import init\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "# Custom convolutional layer\n",
    "def conv(in_planes, out_planes, kernel_size=3, stride=1, dilation=1, bias=False, transposed=False):\n",
    "    if transposed:\n",
    "        layer = nn.ConvTranspose2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=1, output_padding=1,\n",
    "                                   dilation=dilation, bias=bias)\n",
    "    else:\n",
    "        padding = (kernel_size + 2 * (dilation - 1)) // 2\n",
    "        layer = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)\n",
    "    if bias:\n",
    "        init.constant_(layer.bias, 0)\n",
    "    return layer\n",
    "\n",
    "# Returns 2D batch normalization layer\n",
    "def bn(planes):\n",
    "    layer = nn.BatchNorm2d(planes)\n",
    "    init.constant_(layer.weight, 1)\n",
    "    init.constant_(layer.bias, 0)\n",
    "    return layer\n",
    "\n",
    "# Feature extraction using pretrained residual network\n",
    "class FeatureResNet(ResNet):\n",
    "    def __init__(self):\n",
    "        super().__init__(BasicBlock, [3, 14, 16, 3], 1000)\n",
    "        self.conv_f = conv(2, 64, kernel_size=3, stride=1)\n",
    "        self.ReLu_1 = nn.ReLU(inplace=True)\n",
    "        self.conv_pre = conv(512, 1024, stride=2, transposed=False)\n",
    "        self.bn_pre = bn(1024)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv_f(x)  # Convolutional layer\n",
    "        x = self.bn1(x1)\n",
    "        x = self.relu(x)\n",
    "        x2 = self.maxpool(x)  # Max pool layer\n",
    "        x = self.layer1(x2)   # Residual layer 1\n",
    "        x3 = self.layer2(x)   # Residual layer 2\n",
    "        x4 = self.layer3(x3)  # Residual layer 3\n",
    "        x5 = self.layer4(x4)  # Residual layer 4\n",
    "        x6 = self.ReLu_1(self.bn_pre(self.conv_pre(x5)))  # Convolutional layer\n",
    "        return x1, x2, x3, x4, x5, x6\n",
    "\n",
    "# Segmentation network\n",
    "class SegResNet(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained_net):\n",
    "        super().__init__()\n",
    "        self.pretrained_net = pretrained_net\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv3_2 = conv(1024, 512, stride=1, transposed=False)\n",
    "        self.bn3_2 = bn(512)\n",
    "        self.conv4 = conv(512, 512, stride=2, transposed=True)\n",
    "        self.bn4 = bn(512)\n",
    "        self.conv5 = conv(512, 256, stride=2, transposed=True)\n",
    "        self.bn5 = bn(256)\n",
    "        self.conv6 = conv(256, 128, stride=2, transposed=True)\n",
    "        self.bn6 = bn(128)\n",
    "        self.conv7 = conv(128, 64, stride=2, transposed=True)\n",
    "        self.bn7 = bn(64)\n",
    "        self.conv8 = conv(64, 64, stride=2, transposed=True)\n",
    "        self.bn8 = bn(64)\n",
    "        self.conv9 = conv(64, 32, stride=2, transposed=True)\n",
    "        self.bn9 = bn(32)\n",
    "        self.convadd = conv(32, 16, stride=1, transposed=False)\n",
    "        self.bnadd = bn(16)\n",
    "        self.conv10 = conv(16, num_classes, stride=2, kernel_size=5)\n",
    "        init.constant_(self.conv10.weight, 0)  # Zero init\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, x2, x3, x4, x5, x6 = self.pretrained_net(x)  # Feature extraction\n",
    "        \n",
    "        x = self.relu(self.bn3_2(self.conv3_2(x6)))\n",
    "        x = self.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.relu(self.bn5(self.conv5(x)))\n",
    "        x = self.relu(self.bn6(self.conv6(x + x4)))\n",
    "        x = self.relu(self.bn7(self.conv7(x + x3)))\n",
    "        x = self.relu(self.bn8(self.conv8(x + x2)))\n",
    "        x = self.relu(self.bn9(self.conv9(x + x1)))\n",
    "        x = self.relu(self.bnadd(self.convadd(x)))\n",
    "        x = self.conv10(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model_path = 'trained_model/model/param_all_2_99_742024_07_25_12-34-03'\n",
    "\n",
    "fnet = FeatureResNet()\n",
    "fcn = SegResNet(2, fnet)\n",
    "fcn = fcn.cpu()\n",
    "\n",
    "fcn.load_state_dict(torch.load(saved_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'google-dataset/speckle-image-dataset/'\n",
    "test_count = 512\n",
    "train_count = 4096\n",
    "            \n",
    "test_set = []\n",
    "for z in range(0, test_count):\n",
    "    test_set.append((folder + 'imgs3/train_image_' + str(z+1)+'_1.png',\n",
    "                       folder + 'imgs3/train_image_' + str(z+1)+'_2.png',\n",
    "                       folder + 'gt3/train_image_' + str(z+1)+'.mat'))\n",
    "            \n",
    "train_set = []\n",
    "for z in range(test_count, train_count):\n",
    "    train_set.append((folder + 'imgs3/train_image_' + str(z+1)+'_1.png',\n",
    "                       folder + 'imgs3/train_image_' + str(z+1)+'_2.png',\n",
    "                       folder + 'gt3/train_image_' + str(z+1)+'.mat'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('google-dataset/speckle-image-dataset/imgs3/train_image_11_1.png', 'google-dataset/speckle-image-dataset/imgs3/train_image_11_2.png', 'google-dataset/speckle-image-dataset/gt3/train_image_11.mat')\n"
     ]
    }
   ],
   "source": [
    "print(train_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import random\n",
    "\n",
    "class MyDataset(data_utils.Dataset):\n",
    "    def __init__(self, dataset, transform=None, target_transform=None, loader=default_loader):\n",
    "        self.imgs = dataset\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.loader = loader\n",
    "\n",
    "    def random_transform(self, img):\n",
    "        # Initialize the transformation sequence\n",
    "        transform_seq = transforms.Compose([\n",
    "            # Apply random rotation with a probability\n",
    "            # transforms.RandomApply([\n",
    "            #     transforms.RandomRotation(degrees=(-10, 10))\n",
    "            # ], p=0.5),\n",
    "\n",
    "            # Apply random zoom with a probability of 0.5\n",
    "            # transforms.RandomApply([\n",
    "            #     transforms.RandomAffine(degrees=0, scale=(0.95, 1.05))\n",
    "            # ], p=0.5),\n",
    "\n",
    "            # Randomly adjust brightness, contrast, and saturation\n",
    "            transforms.RandomApply([\n",
    "                transforms.ColorJitter(brightness=(0.5, 1.5), contrast=(0.5, 1.5), saturation=(0.5, 1.5))\n",
    "            ], p=0.5),\n",
    "        ])\n",
    "        \n",
    "        # Apply the transformation sequence\n",
    "        return transform_seq(img)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label_x, label_y, label_z = self.imgs[index]\n",
    "        img1 = self.loader(label_x)\n",
    "        img1 = self.random_transform(img1)\n",
    "        img_1 = ToTensor()(img1.resize((128,128)))\n",
    "        img_1 = img_1[::4,:,:]\n",
    "\n",
    "        img2 = self.loader(label_y)\n",
    "        img2 = self.random_transform(img2)\n",
    "        img_2 = ToTensor()(img2.resize((128,128)))\n",
    "        img_2 = img_2[::4,:,:]\n",
    "        \n",
    "        imgs = torch.cat((img_1, img_2), 0)\n",
    "        \n",
    "        try:\n",
    "            gt = sio.loadmat(label_z)['Disp_field_1'].astype(float)\n",
    "        except KeyError:\n",
    "            gt = sio.loadmat(label_z)['Disp_field_2'].astype(float)\n",
    "        \n",
    "        gt = torch.tensor(gt).permute(2, 0, 1)  # Ensure gt is [C, H, W]\n",
    "        gt = nn.functional.interpolate(gt.unsqueeze(0), size=(128, 128), mode='bilinear', align_corners=False).squeeze(0)  # Resize to [C, 128, 128]\n",
    "        \n",
    "        return imgs, gt\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TROUBLESHOOTING\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_SIZE =  64\n"
     ]
    }
   ],
   "source": [
    "# a simple custom collate function, just to show the idea\n",
    "def my_collate(batch):\n",
    "    data = [item[0] for item in batch]\n",
    "    target = [item[1] for item in batch]\n",
    "    \n",
    "    # Check if all images have the same size\n",
    "    sizes_data = [img.size() for img in data]\n",
    "    if not all(size == sizes_data[0] for size in sizes_data):\n",
    "        raise ValueError(\"All images in a batch must have the same size.\")\n",
    "    \n",
    "    # Check if all ground truth tensors have the same size\n",
    "    sizes_target = [gt.size() for gt in target]\n",
    "    if not all(size == sizes_target[0] for size in sizes_target):\n",
    "        raise ValueError(\"All ground truth tensors in a batch must have the same size.\")\n",
    "    \n",
    "    target = torch.stack(target, 0)\n",
    "    return [data, target]\n",
    "    \n",
    "EPOCH = 40\n",
    "BATCH_SIZE = 64\n",
    "print('BATCH_SIZE = ',BATCH_SIZE)\n",
    "LR = 0.001              # learning rate\n",
    "# too big: converges fast but misses local minimal. Small: time consuming. Use array: every 20 steps LR *= 0.9. \n",
    "NUM_WORKERS = 0\n",
    "\n",
    "optimizer = torch.optim.Adam(fcn.parameters(), lr=LR)   # optimize all cnn parameters\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9) # dynamic learning rate\n",
    "#optimizer = torch.optim.SGD(cnn.parameters(), lr=LR, momentum=0.9)   # optimize all cnn parameters\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "\n",
    "train_data=MyDataset(dataset=train_set)\n",
    "train_loader = data_utils.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "test_data=MyDataset(dataset=test_set)\n",
    "test_loader = data_utils.DataLoader(dataset=test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "dataString = datetime.strftime(datetime.now(), '%Y_%m_%d_%H-%M-%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_result = 'trained_model/'\n",
    "if not os.path.exists(root_result):\n",
    "    os.mkdir(root_result)\n",
    "\n",
    "model_result = os.path.join(root_result, 'model/')\n",
    "log_result = os.path.join(root_result, 'log/')\n",
    "\n",
    "if not os.path.exists(model_result):\n",
    "    os.mkdir(model_result)\n",
    "\n",
    "if not os.path.exists(log_result):\n",
    "    os.mkdir(log_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time: 2024-07-30 15:49:12.106443\n",
      "epoch: 0, batch step: 0, loss: 0.27605512738227844\n",
      "2024-07-30 15:49:44.071385 (0:0:31)\n",
      "epoch: 0, batch step: 1, loss: 0.7177653908729553\n",
      "2024-07-30 15:49:56.004633 (0:0:43)\n",
      "Epoch 0: Average Train Loss: 0.4969102591276169\n",
      "\n",
      "Epoch 0: Average Training Loss: 0.4969102591276169\n",
      "Epoch 0, Current learning rate: [0.001]\n",
      "validation error epoch  0:    tensor(0.3548)\n",
      "9\n",
      "epoch: 1, batch step: 0, loss: 0.2839232385158539\n",
      "2024-07-30 15:50:31.634508 (0:1:19)\n",
      "epoch: 1, batch step: 1, loss: 0.34490251541137695\n",
      "2024-07-30 15:50:43.401739 (0:1:31)\n",
      "Epoch 1: Average Train Loss: 0.3144128769636154\n",
      "\n",
      "Epoch 1: Average Training Loss: 0.3144128769636154\n",
      "Epoch 1, Current learning rate: [0.001]\n",
      "validation error epoch  1:    tensor(1.6309)\n",
      "9\n",
      "epoch: 2, batch step: 0, loss: 0.33376118540763855\n",
      "2024-07-30 15:51:28.134261 (0:2:16)\n",
      "epoch: 2, batch step: 1, loss: 0.2612926661968231\n",
      "2024-07-30 15:51:45.885607 (0:2:33)\n",
      "Epoch 2: Average Train Loss: 0.29752692580223083\n",
      "\n",
      "Epoch 2: Average Training Loss: 0.29752692580223083\n",
      "Epoch 2, Current learning rate: [0.001]\n",
      "validation error epoch  2:    tensor(0.7123)\n",
      "9\n",
      "epoch: 3, batch step: 0, loss: 0.22405344247817993\n",
      "2024-07-30 15:52:33.013591 (0:3:20)\n",
      "epoch: 3, batch step: 1, loss: 0.49059244990348816\n",
      "2024-07-30 15:52:51.465825 (0:3:39)\n",
      "Epoch 3: Average Train Loss: 0.35732294619083405\n",
      "\n",
      "Epoch 3: Average Training Loss: 0.35732294619083405\n",
      "Epoch 3, Current learning rate: [0.001]\n",
      "validation error epoch  3:    tensor(0.3906)\n",
      "9\n",
      "epoch: 4, batch step: 0, loss: 0.21728967130184174\n",
      "2024-07-30 15:53:37.894802 (0:4:25)\n",
      "epoch: 4, batch step: 1, loss: 0.24330051243305206\n",
      "2024-07-30 15:53:56.441195 (0:4:44)\n",
      "Epoch 4: Average Train Loss: 0.2302950918674469\n",
      "\n",
      "Epoch 4: Average Training Loss: 0.2302950918674469\n",
      "Epoch 4, Current learning rate: [0.001]\n",
      "validation error epoch  4:    tensor(0.2858)\n",
      "9\n",
      "epoch: 5, batch step: 0, loss: 0.1561851054430008\n",
      "2024-07-30 15:54:43.119862 (0:5:31)\n",
      "epoch: 5, batch step: 1, loss: 0.21435409784317017\n",
      "2024-07-30 15:55:01.248935 (0:5:49)\n",
      "Epoch 5: Average Train Loss: 0.18526960164308548\n",
      "\n",
      "Epoch 5: Average Training Loss: 0.18526960164308548\n",
      "Epoch 5, Current learning rate: [0.001]\n",
      "validation error epoch  5:    tensor(0.5150)\n",
      "9\n",
      "epoch: 6, batch step: 0, loss: 0.20953701436519623\n",
      "2024-07-30 15:55:50.641839 (0:6:38)\n",
      "epoch: 6, batch step: 1, loss: 0.32352012395858765\n",
      "2024-07-30 15:56:06.008368 (0:6:53)\n",
      "Epoch 6: Average Train Loss: 0.26652856916189194\n",
      "\n",
      "Epoch 6: Average Training Loss: 0.26652856916189194\n",
      "Epoch 6, Current learning rate: [0.001]\n",
      "validation error epoch  6:    tensor(0.3968)\n",
      "9\n",
      "epoch: 7, batch step: 0, loss: 0.2043137550354004\n",
      "2024-07-30 15:56:45.957878 (0:7:33)\n",
      "epoch: 7, batch step: 1, loss: 0.137850821018219\n",
      "2024-07-30 15:57:01.449707 (0:7:49)\n",
      "Epoch 7: Average Train Loss: 0.1710822880268097\n",
      "\n",
      "Epoch 7: Average Training Loss: 0.1710822880268097\n",
      "Epoch 7, Current learning rate: [0.001]\n",
      "validation error epoch  7:    tensor(0.2407)\n",
      "9\n",
      "epoch: 8, batch step: 0, loss: 0.17879252135753632\n",
      "2024-07-30 15:57:40.941192 (0:8:28)\n",
      "epoch: 8, batch step: 1, loss: 0.2280828207731247\n",
      "2024-07-30 15:57:55.707814 (0:8:43)\n",
      "Epoch 8: Average Train Loss: 0.2034376710653305\n",
      "\n",
      "Epoch 8: Average Training Loss: 0.2034376710653305\n",
      "Epoch 8, Current learning rate: [0.001]\n",
      "validation error epoch  8:    tensor(0.1841)\n",
      "9\n",
      "epoch: 9, batch step: 0, loss: 0.11185561865568161\n",
      "2024-07-30 15:58:35.757870 (0:9:23)\n",
      "epoch: 9, batch step: 1, loss: 0.11038947105407715\n",
      "2024-07-30 15:58:50.589665 (0:9:38)\n",
      "Epoch 9: Average Train Loss: 0.11112254485487938\n",
      "\n",
      "Finished saving checkpoints for epoch 9\n",
      "Epoch 9: Average Training Loss: 0.11112254485487938\n",
      "Epoch 9, Current learning rate: [0.0009000000000000001]\n",
      "validation error epoch  9:    tensor(0.1169)\n",
      "9\n",
      "epoch: 10, batch step: 0, loss: 0.16073249280452728\n",
      "2024-07-30 15:59:29.779517 (0:10:17)\n",
      "epoch: 10, batch step: 1, loss: 0.12963344156742096\n",
      "2024-07-30 15:59:44.870622 (0:10:32)\n",
      "Epoch 10: Average Train Loss: 0.14518296718597412\n",
      "\n",
      "Epoch 10: Average Training Loss: 0.14518296718597412\n",
      "Epoch 10, Current learning rate: [0.0009000000000000001]\n",
      "validation error epoch  10:    tensor(0.2052)\n",
      "9\n",
      "epoch: 11, batch step: 0, loss: 0.10046755522489548\n",
      "2024-07-30 16:00:23.740557 (0:11:11)\n",
      "epoch: 11, batch step: 1, loss: 0.12182489782571793\n",
      "2024-07-30 16:00:38.557127 (0:11:26)\n",
      "Epoch 11: Average Train Loss: 0.1111462265253067\n",
      "\n",
      "Epoch 11: Average Training Loss: 0.1111462265253067\n",
      "Epoch 11, Current learning rate: [0.0009000000000000001]\n",
      "validation error epoch  11:    tensor(0.3388)\n",
      "9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(output, gt)  \u001b[38;5;66;03m# cross entropy loss\u001b[39;00m\n\u001b[0;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# clear gradients for this training step\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# backpropagation, compute gradients\u001b[39;00m\n\u001b[0;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# apply gradients\u001b[39;00m\n\u001b[0;32m     35\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# Accumulate the loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\vscode\\2024-IS2EE-Research\\researchproject_deepDIC\\.venv\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\vscode\\2024-IS2EE-Research\\researchproject_deepDIC\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\vscode\\2024-IS2EE-Research\\researchproject_deepDIC\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "fileOut=open(log_result+'log_'+dataString,'a')\n",
    "fileOut.write(dataString+'Epoch:   Step:    Loss:        Val_Accu :\\n')\n",
    "fileOut.close()\n",
    "fileOut2 = open(log_result+'validation_'+dataString, 'a')\n",
    "fileOut2.write('kernal_size of conv_f is 2')\n",
    "fileOut2.write(dataString+'Epoch:    loss:')\n",
    "fileOut3 = open(log_result+'train_'+dataString, 'a')\n",
    "fileOut3.write('kernal_size of conv_f is 2')\n",
    "fileOut3.write(dataString+'Epoch:    loss:')\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(\"Start Time:\", start_time)\n",
    "\n",
    "with open(log_result + 'elapsed_time_log_' + dataString, 'a') as fileOut:\n",
    "    fileOut.write(f\"Start time: {start_time}\\n\")\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    fcn.train()\n",
    "    total_loss = 0  # Initialize the loss accumulator\n",
    "    num_batches = 0  # Counter for the number of batches\n",
    "\n",
    "    for step, (img, gt) in enumerate(train_loader):   # gives batch data, normalize x when iterate train_loader\n",
    "        img = Variable(img).cpu()\n",
    "        gt = gt.float()\n",
    "        gt = Variable(gt).cpu()\n",
    "        output = fcn(img)  # cnn output\n",
    "        loss = loss_func(output, gt)  # cross entropy loss\n",
    "        optimizer.zero_grad()  # clear gradients for this training step\n",
    "        loss.backward()  # backpropagation, compute gradients\n",
    "        optimizer.step()  # apply gradients\n",
    "\n",
    "        total_loss += loss.item()  # Accumulate the loss\n",
    "        num_batches += 1\n",
    "\n",
    "        print(f\"epoch: {epoch}, batch step: {step}, loss: {loss.data.item()}\")\n",
    "        with open(log_result + 'log_' + dataString, 'a') as fileOut:\n",
    "            fileOut.write(f\"{epoch}   {step}   {loss.data.item()}\\n\")\n",
    "\n",
    "        end_time = datetime.now()\n",
    "\n",
    "        elapsed_time = end_time - start_time\n",
    "        elapsed_hours, remainder = divmod(elapsed_time.total_seconds(), 3600)\n",
    "        elapsed_minutes, elapsed_seconds = divmod(remainder, 60)\n",
    "\n",
    "        # Output total elapsed time in hours:minutes:seconds\n",
    "        print(\"{} ({}:{}:{})\".format(\n",
    "            end_time, int(elapsed_hours), int(elapsed_minutes), int(elapsed_seconds)\n",
    "        ))\n",
    "        \n",
    "        with open(log_result + 'elapsed_time_log_' + dataString, 'a') as fileOut:\n",
    "            fileOut.write(f\"{epoch}   {step}   {end_time}   {elapsed_hours}:{elapsed_minutes}:{elapsed_seconds}\\n\")\n",
    "\n",
    "    average_loss = total_loss / num_batches  # Calculate average loss for the epoch\n",
    "    with open(log_result + 'train_' + dataString, 'a') as fileOut:\n",
    "        fileOut.write(f\"Epoch {epoch}: Average Loss: {average_loss}\\n\")  # Log the average loss per epoch\n",
    "        print(f\"Epoch {epoch}: Average Train Loss: {average_loss}\\n\")\n",
    "\n",
    "    if epoch % 10 == 9:\n",
    "        PATH = model_result + 'model' + dataString + '_' + str(epoch) + '_' + str(step)\n",
    "        torch.save(fcn.state_dict(), PATH)\n",
    "        print(f'Finished saving checkpoints for epoch {epoch}')\n",
    "\n",
    "    print(f'Epoch {epoch}: Average Training Loss: {average_loss}')\n",
    "    \n",
    "    scheduler.step()\n",
    "    print(\"Epoch {}, Current learning rate: {}\".format(epoch, scheduler.get_last_lr()))\n",
    "     \n",
    "    LOSS_VALIDATION = 0\n",
    "    fcn.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, (img,gt) in enumerate(test_loader):\n",
    "\n",
    "            img = Variable(img).cpu()\n",
    "            # gt=gt.unsqueeze(1)# batch x\n",
    "            gt=gt.float()\n",
    "            gt = Variable(gt).cpu()\n",
    "            # print(f\"gt test size:{gt.size()}\")\n",
    "            output = fcn(img) \n",
    "            # print(f\"validation output size:{output.size()}\")\n",
    "            LOSS_VALIDATION += loss_func(output, gt)\n",
    "        #print(LOSS_VALIDATION.data.item())\n",
    "        LOSS_VALIDATION = LOSS_VALIDATION/step\n",
    "        fileOut2 = open(log_result+'validation_'+dataString, 'a')\n",
    "        fileOut2.write(str(epoch)+'   '+str(step)+'   '+str(LOSS_VALIDATION.data.item())+'\\n')\n",
    "        fileOut2.close()\n",
    "        print('validation error epoch  '+str(epoch)+':    '+str(LOSS_VALIDATION)+'\\n'+str(step))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
